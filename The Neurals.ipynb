{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "import re\n",
    "#unaccented_string = unidecode.unidecode(accented_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/winemag-data-130k-v2.csv\", encoding='utf-8')\n",
    "data.drop([data.columns[0], 'designation', 'taster_twitter_handle'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    try: \n",
    "        return unidecode.unidecode(x).lower() \n",
    "    except: \n",
    "        return x\n",
    "for col in data.columns:\n",
    "    data[col] = data[col].apply(func)\n",
    "data['description'] = data['description'].apply(lambda(x): re.sub(\"[^a-zA-Z ]\",\"\", re.sub(\"-\", \" \", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = list(data['variety'].value_counts()[0:30].index)\n",
    "def label_row(row):\n",
    "    if row['variety'] in chosen:\n",
    "        return row['variety']\n",
    "    return 'other'\n",
    "data['y_variety'] = data.apply (lambda row: label_row(row),axis=1)\n",
    "chosen = list(data['province'].value_counts()[0:30].index)\n",
    "def label_row(row):\n",
    "    if row['province'] in chosen:\n",
    "        return row['province']\n",
    "    return 'other'\n",
    "data['y_province'] = data.apply (lambda row: label_row(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "trainRaw = data.sample(n=100000, replace=False, random_state=1)\n",
    "test_val = data.drop(trainRaw.index)\n",
    "testRaw = test_val.sample(frac=0.5, replace=False, random_state=1)\n",
    "valRaw = test_val.drop(testRaw.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRaw.to_json(\"train.json\",orient='records')\n",
    "testRaw.to_json(\"test.json\",orient='records')\n",
    "valRaw = test_val.drop(testRaw.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.load(\"data/GloVe_wine_5k.npy\")\n",
    "words = np.load('data/5k_vocab_dict.npy').item()\n",
    "EMBEDDING_DIM = len(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "for k,v in words.items():\n",
    "    embedding_dict[k] = vectors[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(trainRaw['description'])\n",
    "texts_val = list(valRaw['description'])\n",
    "labels = list(trainRaw['y_variety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=136\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(vectors))\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences_val = tokenizer.texts_to_sequences(texts_val)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_val = pad_sequences(sequences_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(trainRaw['y_variety']))\n",
    "labels = le.transform(trainRaw['y_variety'])\n",
    "labels_val = le.transform(valRaw['y_variety'])\n",
    "keys = list(le.classes_)\n",
    "vals = le.transform(keys)\n",
    "labels_index = dict(zip(keys,vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 14985 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 133s 1ms/step - loss: 2.1872 - acc: 0.3482 - val_loss: 1.8776 - val_acc: 0.4257\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 132s 1ms/step - loss: 1.7538 - acc: 0.4612 - val_loss: 1.7489 - val_acc: 0.4576\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 144s 1ms/step - loss: 1.6118 - acc: 0.5016 - val_loss: 1.6729 - val_acc: 0.4898\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 134s 1ms/step - loss: 1.5149 - acc: 0.5297 - val_loss: 1.6787 - val_acc: 0.4938\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 886s 9ms/step - loss: 1.4352 - acc: 0.5530 - val_loss: 1.6732 - val_acc: 0.5021\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 746s 7ms/step - loss: 1.3612 - acc: 0.5725 - val_loss: 1.6854 - val_acc: 0.5016\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 164s 2ms/step - loss: 1.2954 - acc: 0.5927 - val_loss: 1.7243 - val_acc: 0.5003\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 171s 2ms/step - loss: 1.2328 - acc: 0.6112 - val_loss: 1.7510 - val_acc: 0.4922\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 146s 1ms/step - loss: 1.1721 - acc: 0.6296 - val_loss: 1.7964 - val_acc: 0.4944\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 150s 2ms/step - loss: 1.1148 - acc: 0.6466 - val_loss: 1.8438 - val_acc: 0.4906\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "encoding = to_categorical(labels)\n",
    "encoding_val = to_categorical(labels_val)\n",
    "res = model.fit(data, encoding, validation_data=(data_val, encoding_val),\n",
    "          epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 136, 200)          5855600   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 31)                3131      \n",
      "=================================================================\n",
      "Total params: 5,979,131\n",
      "Trainable params: 123,531\n",
      "Non-trainable params: 5,855,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 100000 samples, validate on 14985 samples\n",
      "Epoch 1/6\n",
      "100000/100000 [==============================] - 273s 3ms/step - loss: 1.7869 - acc: 0.4480 - val_loss: 1.3689 - val_acc: 0.5685\n",
      "Epoch 2/6\n",
      "100000/100000 [==============================] - 274s 3ms/step - loss: 1.2773 - acc: 0.5966 - val_loss: 1.2120 - val_acc: 0.6159\n",
      "Epoch 3/6\n",
      "100000/100000 [==============================] - 237s 2ms/step - loss: 1.1524 - acc: 0.6320 - val_loss: 1.1583 - val_acc: 0.6309\n",
      "Epoch 4/6\n",
      "100000/100000 [==============================] - 243s 2ms/step - loss: 1.0775 - acc: 0.6535 - val_loss: 1.1341 - val_acc: 0.6376\n",
      "Epoch 5/6\n",
      "100000/100000 [==============================] - 253s 3ms/step - loss: 1.0196 - acc: 0.6720 - val_loss: 1.1075 - val_acc: 0.6462\n",
      "Epoch 6/6\n",
      "100000/100000 [==============================] - 254s 3ms/step - loss: 0.9675 - acc: 0.6879 - val_loss: 1.0935 - val_acc: 0.6522\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "encoding = to_categorical(labels)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "results = model.fit(data, encoding, epochs=6, validation_data=(data_val, encoding_val), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9994, 13)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valRaw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
